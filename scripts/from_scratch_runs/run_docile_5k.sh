#!/bin/bash
#SBATCH --job-name=MVLM_docile_5K
#SBATCH --output=out_%j.txt
#SBATCH --error=err_%j.txt
#SBATCH --constraint v100-32g
#SBATCH --hint=multithread 
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=20
#SBATCH --time=10:00:00
#SBATCH -A zke@v100

MVLM_DATASET=docile_5k
MVLM_VAL_DATASET=docile
TOKEN_CLASSIF_DATASET=docile
IS_DOCILE=true
SUBSET_INDEX_PATH=data/docile/subsets_index/5k.json
HOME_DIR=$SCRATCH/DeepInsuranceDocs  # To adjust

HF_MODEL_DIR=$HOME_DIR/models/huggingface
PRETRAINED_MODEL=null
PREPROCESS_NORMALIZE_TXT=true
PREPROCESS_TAG_SCHEME=BIO

EPOCH_NUM=5
BATCH_SIZE=10
LEARNING_RATE=5e-5
GRADIENT_ACCUMULATION_STEPS=0
MVLM_CONFIG_PATH=experiments/from_scratch_mvlm_docile_5k.json

# set -x

echo "MVLM_DATASET: $MVLM_DATASET"
echo "TOKEN_CLASSIF_DATASET: $TOKEN_CLASSIF_DATASET"
echo "SUBSET_INDEX_PATH: $SUBSET_INDEX_PATH"
echo "HOME_DIR: $HOME_DIR"
echo "HF_MODEL_DIR: $HF_MODEL_DIR"
echo "PRETRAINED_MODEL: $PRETRAINED_MODEL"
echo "PREPROCESS_NORMALIZE_TXT: $PREPROCESS_NORMALIZE_TXT"
echo "PREPROCESS_TAG_SCHEME: $PREPROCESS_TAG_SCHEME"
echo "EPOCH_NUM: $EPOCH_NUM"
echo "BATCH_SIZE: $BATCH_SIZE"
echo "LEARNING_RATE: $LEARNING_RATE"
echo "GRADIENT_ACCUMULATION_STEPS: $GRADIENT_ACCUMULATION_STEPS"
echo "MVLM_CONFIG_PATH: $MVLM_CONFIG_PATH"

# Run MVLM on docile_5k without pretraining
bash scripts/run_pretraining.sh\
    MVLM_DATASET=$MVLM_DATASET\
    MVLM_VAL_DATASET=$MVLM_VAL_DATASET\
    TOKEN_CLASSIF_DATASET=$TOKEN_CLASSIF_DATASET\
    IS_DOCILE=$IS_DOCILE\
    SUBSET_INDEX_PATH=$SUBSET_INDEX_PATH\
    HOME_DIR=$HOME_DIR\
    HF_MODEL_DIR=$HF_MODEL_DIR\
    PRETRAINED_MODEL=$PRETRAINED_MODEL\
    PREPROCESS_NORMALIZE_TXT=$PREPROCESS_NORMALIZE_TXT\
    PREPROCESS_TAG_SCHEME=$PREPROCESS_TAG_SCHEME\
    EPOCH_NUM=$EPOCH_NUM\
    BATCH_SIZE=$BATCH_SIZE\
    LEARNING_RATE=$LEARNING_RATE\
    GRADIENT_ACCUMULATION_STEPS=$GRADIENT_ACCUMULATION_STEPS\
    MVLM_CONFIG_PATH=$MVLM_CONFIG_PATH

# Run Token Classif on docile with model pretrained on docile_5k
